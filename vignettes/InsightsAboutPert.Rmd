---
title: "Insights about Pert"
author: "Dr. Jan Seifert"
date: "05-04-2021"
output: html_document
bibliography: ["pert.bib"]
#biblio-style: "apalike"
csl: apa-6th-edition.csl
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("../R/threepoint_core.R")
```

This note book tries to solve some puzzles regarding the Pert distribution. There is a lot of discussion in the literature regarding some shortcomings of the Pert distribution. Mainly because the standard deviation would not be fixed to 1/6th like the original authors claimed [@US1958].

For this investigation, assume a Pert distribution with a range from $a$ to $c$ and a mode of $b$.



# Understanding the Problem

The Pert probability distribution is essentially a special case of the Beta distribution. 

## Mean

Every source says that the mean of the Pert distribution was:

$$
\begin{equation}
  \operatorname{E}[X]= \frac{a+4b+c}{6} = \frac{a+\lambda \cdot b+c}{\lambda+2} = \mu
\end{equation}
$$

Sometimes, an additional shape argument λ is used to which @Vose2008 refers as "modified Pert". He suggests that estimates can be improved if subject matter experts do not only estimate range and mode of the distribution but the distribution shape, as well. For the classic Pert distribution $\lambda = 4$.

Strangely, this mean is also the same equation of the [double triangular distribution](http://www.mhnederlof.nl/doubletriangular.html) [as noted by @Henry27280]. @Henry27280 speculates that the Pert mean would come from the double triangular. However, I couldn't find any reason or source that justifies the connection for that and it seems like a coincidence.

Looking closer at the mathematics it becomes clear that the Beta distribution cannot guarantee the specified mean. It is a special case of the beta distribution. The general formula for the mean of the $beta(\alpha, \beta, a, c)$ distribution is [@enwiki1011418887; @Treat1983]:

$$
\mu = \frac\alpha{\alpha+\beta} \cdot (c-a) + a = \frac{\alpha c + \beta a}{\alpha+\beta}
$$

So, it is clear that it is not at all determined by the max/min parameters. It is also influenced by two parameters α and β. Finding the right parameters is then crucial.


## Standard Deviation

The standard deviation of the Pert distribution is known as one sixth of it's total range.

$$ \sigma = {c - a \over 6} = \frac{1}{6}(c - a)$$

However, that is an approximation. The exact formula for the variance is [@enwiki1001556353]:

$$
\sigma^2 = \frac{(\mu-a)(c-\mu)}{7}
$$



For demonstration purposes let us simplify this by setting the min $a=0$ and the max value $c=1$. That leaves:

$$
\sigma^2 = 
\frac{1}{36} \cdot \frac{- 16b^2 + 16b + 5}{7} = \frac{1}{36} \cdot \frac{5 - 16 (b - 1) b}{7}
$$

```{r Plot Variances of Beta, echo=FALSE}
library(ggplot2)
b <- seq(0, 1, 0.05)
Variance <- (5-16*(b-1)*b) / 7 / 36
data <- data.frame(b, Variance)
ggplot(data, aes(x = b, y = Variance)) + 
  geom_line() +
  geom_hline(yintercept=1/36, linetype="dashed", color = "darkgray") +
  ggtitle("True Standard Deviation of the Beta Distribution") +
  scale_x_continuous(name="Mode b") + scale_y_continuous(name="Variance") +
  theme_linedraw()
```

As we can see the variance depends on the most likely estimate $b$. It ranges around 1/6 = `r format(1/6, digits=2, nsmall = 3)` (shown by the dashed line) from `r format(min(Variance), digits=2, nsmall = 3)` to `r format(max(Variance), digits=2, nsmall = 3)`. Such variations are why @Farnum1987 warned about using the approximation in extreme cases when the most-likely estimate is close to the minimum or maximum, i. e. close than 13% of the total range.


However, the above equations have been set up under one assumption: a certain way to calculate the α and β shape parameters of the beta distribution.

In terms of the beta distribution the variance is:

$$ \text{var}(X)(c-a)^2 = 
    \frac{\alpha\beta (c-a)^2} {(\alpha+\beta)^2(\alpha+\beta+1)} $$

Again it is easy to see how the shape parameters of the beta distribution are related to the variance of the distribution. The α and β parameters of the beta distribution determine mean and variance of this distribution. Everything stated above depends on that. Hence, further investigations are required.





## Synthesis?

How can these discrepancies be addressed? I found three approaches in the literature.

 1. Find ways to parametrize the beta distribution exactly to guarantee the defined mean and standard deviation [@Davis2008].
 2. Find new equations that give the correct (or a more correct) mean and standard deviation [e.g. @Lau1998; @GolenkoGinzburg1988; summarized and challenged by @Perez2017].
 3. Yet others investigate the relation between mean/standard deviation and the shape parameter of the Pert distribution [see @Pleguezuelo2003].

Only the first solution seems to be something other than a crutch. If we could really find a way to have the fixed standard deviation why should we not go for it? If we solve the equations for a simple example it becomes clear that there is a discrepancy. The plot shows the variances (y-axis) depending on mode (x-axis), and the parametrization method (colour). The variances of Davis and Vose have been moved a little bit to the right because it would not be possible to distinguish those from other equations.

```{r Plot Deviation from Variance, echo=FALSE}
library(ggplot2)
min <- -1
max <- +1
shape <- 4

mode <- seq(min, max, 0.05)

Data <- data.frame(Mode = numeric(), 
                   Mean = numeric(), 
                   Var = numeric(), 
                   Method = character())
Jitter  <- c(classic = -0.02, golgin = 0.00, vose = 0.02, davis = -0.01, Theoretical = 0.01)

# Compute using different parametrization methods
for (m in c("classic", "golgin", "vose", "davis")) {
  Params <- Pert2BetaParams(min, mode, max, shape, m)
  tMean  <- Params$alpha / (Params$alpha+Params$beta) * (max-min) + min
  tVar   <- (max-min)^2 * Params$alpha * Params$beta / 
            (Params$alpha+Params$beta)^2 / (Params$alpha+Params$beta+1)
  Data <- rbind(Data, data.frame(Mode = mode + Jitter[m], 
                                 Mean = tMean, 
                                 Var  = tVar, 
                                 Method = rep(m, length(mode))))
}

# Add theoretical values
Data <- rbind(Data, data.frame(Mode = mode + Jitter[m], 
                               Mean = (min + shape * mode + max)/(shape + 2), 
                               Var  = (max-min)^2 / 36, 
                               Method = rep("Theoretical", length(mode))))

# Plot
ggplot(Data, aes(x = Mode, y = Var, colour = Method)) +
  geom_line(size = 3/4) + theme_linedraw() + 
  scale_color_manual(values = c("sienna1", "tan4", "hotpink1", "black", "darkmagenta")) + 
  labs(x = "Var", y = "Mode b")
```

As we can see the parametrization @Davis2008 works. He proposes a new way to estimate the α and β parameters of the beta distribution to make sure that mean and standard deviation are fixed to the defined values:

$$   \alpha = \frac{2 \cdot (c + 4b - 5a)} {3(c-a)} \cdot 
       \left[ 1 + 4 \cdot \frac{(b-a)\cdot(c-b)}{(c-a)^2} \right] $$

$$   \beta = \alpha \cdot \frac{5c-4b-a}{c+4b-5a} $$

What is most interesting and baffling is the observation that the classic parametrization of the beta distribution shows the exact same pattern, i.e. no deviation from a fixed variance of 1/36.

```{r Plot Deviation from Mean, echo=FALSE}
ggplot(Data, aes(x = Mode, y = Mean, colour = Method)) +
  geom_line(size = 3/4) + theme_linedraw() + 
  scale_color_manual(values = c("sienna1", "tan4", "hotpink1", "black", "darkmagenta")) + 
  labs(x = "Mode", y = "Mean")
```

A similar experiment with distribution means shows no deviation from the mean. Regardless of the parametrization method. All parametrization methods handle the mean well.

All in all, @Davis2008 handles the inconsistency of incorrect variances. It is probably better to avoid the `pert` distribution family of the [mc2d](https://rdrr.io/cran/mc2d/) [@Pouillot2010a] package. Instead, compute α and β using the equations from @Davis2008 and use the generalized [beta distribution (betagen)](https://rdrr.io/cran/mc2d/man/betagen.html) instead.



## Median

A rough estimate of the median is [@enwiki1011418887]

$$ Median = \frac{(a + 6*b + c)}{8} $$

The actual definition of the median is

$$ I_{\frac {1}{2}}^{[-1]}(\alpha, \beta ) \cdot (c-a) + a $$ 

for which the regularized incomplete beta function

$$ {\displaystyle I_{x}(\alpha,\beta )={\tfrac{1}{2}}}I_x(\alpha,\beta) = \tfrac{1}{2}$$.

There is no general closed-form expression for the median of the beta distribution for arbitrary values of α and β. Yet, it can easily be done in R with `qbeta(0.5, alpha, beta) * (c - a) + a`.

What is even more important is the question what the median has to do with the question of properly parametrizing the Beta distribution? Nothing, it seems.




## Mode

We already identified that we can get mean and standard deviation as defined by Pert if we parametrize the Beta distribution correctly. I finally want to verify that the mode is not changed by that.


```{r Mode Simulations}
library(ggplot2)
Min <- -1
Max <- +1
shape <- 4

stepsize <- 0.01
investwidth <- 0.2 * (Max-Min)
ModeSeq <- seq(Min, Max, 0.05)

ModeData <- data.frame(TargetMode = numeric(), RealMode = numeric(), 
                   Method = character())


for (m in c("classic", "golgin", "vose", "davis")) {
  
  RealMode <- numeric()
  
  for (Mode in ModeSeq) {
    # Check interval around designated mode
    X <- seq(max(Min, Mode-investwidth), min(Max, Mode+investwidth), stepsize)
    
    Params <- Pert2BetaParams(Min, Mode, Max, shape, m)
    Values <- dbeta((X - Min) / (Max - Min), Params$alpha, Params$beta)
    RealMode <- c(RealMode, X[which.max(Values)])
  }
  ModeData <- rbind(ModeData, 
                    data.frame(TargetMode = ModeSeq, 
                               RealMode   = RealMode, 
                               Method     = rep(m, length(ModeSeq))))
}
print(
  ggplot(ModeData, aes(x = TargetMode, y = RealMode, colour = Method)) +
  geom_line(size = 3/4) + theme_linedraw() + 
  scale_color_manual(values = c("sienna1", "tan4", "hotpink1", "black", "darkmagenta")) + 
  labs(x = "Expected Mode", y = "Mode Found")
)
```


Two of the parametrization methods return a distribution exactly replicate the expected modes. These are @GolenkoGinzburg1988 ("golgin") and @Vose2008. The classic and Davis' approach lead to distortions especially at the edges.

In sum, both parametrization methods have their advantages. While Davis' approach stabilizes the variance, Golenko-Ginzburg stabilizes the mode. The overview shows it. The table below shows the how the different parameters mean, mode, and variance deviate from the expected one. Davis' parametrization causes an 11% deviation from the expected mode. The plot above shows that the large deviations can be found at extreme modes, i.e. when the mode is close to the minimum or maximum.

The deviation of the variances from their expected values is stronger. The highest deviation is almost 30% and the average is twice as high. We would have to establish what consequences these deviations have but at a first glance it looks like the version from Davis meets the specifications a lot better.



```{r Table of Deviations, echo=FALSE}
TheoreticalMean <- Data[Data$Method == "Theoretical",]$Mean
TheoreticalVar  <- Data[Data$Method == "Theoretical",]$Var
TheoreticalMode <- ModeData$TargetMode

MaxDeviation <- data.frame(
  Parameter = c("Mean", "Mode", "Variance"),
  Classic = 
    c(max(Data[Data$Method == "classic",]$Mean / TheoreticalMean, na.rm = TRUE),
      max(ModeData[ModeData$Method == "classic",]$RealMode / TheoreticalMode, na.rm = TRUE),
      max(Data[Data$Method == "classic",]$Var / TheoreticalVar, na.rm = TRUE)),
  Davis = 
    c(max(Data[Data$Method == "davis",]$Mean / TheoreticalMean, na.rm = TRUE),
      max(ModeData[ModeData$Method == "davis",]$RealMode / TheoreticalMode, na.rm = TRUE),
      max(Data[Data$Method == "davis",]$Var / TheoreticalVar, na.rm = TRUE)),
  Golenko = 
    c(max(Data[Data$Method == "golgin",]$Mean / TheoreticalMean, na.rm = TRUE),
      max(ModeData[ModeData$Method == "golgin",]$RealMode / TheoreticalMode, na.rm = TRUE),
      max(Data[Data$Method == "golgin",]$Var / TheoreticalVar, na.rm = TRUE)),
  Vose = 
    c(max(Data[Data$Method == "vose",]$Mean / TheoreticalMean, na.rm = TRUE),
      max(ModeData[ModeData$Method == "vose",]$RealMode / TheoreticalMode, na.rm = TRUE),
      max(Data[Data$Method == "vose",]$Var / TheoreticalVar, na.rm = TRUE))
)

MeanDeviation <- data.frame(
  Parameter = c("Mean", "Mode", "Variance"),
  Classic = 
    c(mean(abs(Data[Data$Method == "classic",]$Mean / TheoreticalMean), na.rm = TRUE),
      mean(abs(ModeData[ModeData$Method == "classic",]$RealMode / TheoreticalMode), na.rm = TRUE),
      mean(abs(Data[Data$Method == "classic",]$Var / TheoreticalVar), na.rm = TRUE)),
  Davis = 
    c(mean(abs(Data[Data$Method == "davis",]$Mean / TheoreticalMean), na.rm = TRUE),
      mean(abs(ModeData[ModeData$Method == "davis",]$RealMode / TheoreticalMode), na.rm = TRUE),
      mean(abs(Data[Data$Method == "davis",]$Var / TheoreticalVar), na.rm = TRUE)),
  Golenko = 
    c(mean(abs(Data[Data$Method == "golgin",]$Mean / TheoreticalMean), na.rm = TRUE),
      mean(abs(ModeData[ModeData$Method == "golgin",]$RealMode / TheoreticalMode), na.rm = TRUE),
      mean(abs(Data[Data$Method == "golgin",]$Var / TheoreticalVar), na.rm = TRUE)),
  Vose = 
    c(mean(abs(Data[Data$Method == "vose",]$Mean / TheoreticalMean), na.rm = TRUE),
      mean(abs(ModeData[ModeData$Method == "vose",]$RealMode / TheoreticalMode), na.rm = TRUE),
      mean(abs(Data[Data$Method == "vose",]$Var / TheoreticalVar), na.rm = TRUE))
)

# Format columns
for (clmn in 1:ncol(MaxDeviation))
  if(is.numeric(MaxDeviation[[clmn]])) 
    MaxDeviation[[clmn]] <- (MaxDeviation[[clmn]]-1) * 100

for (clmn in 1:ncol(MeanDeviation))
  if(is.numeric(MeanDeviation[[clmn]])) 
    MeanDeviation[[clmn]] <- (MeanDeviation[[clmn]]-1) * 100

# Output
library(knitr); library(kableExtra); library(dplyr)
kable(rbind(MaxDeviation, MeanDeviation),
      digits = 3L, booktabs = TRUE,
      caption	= "Deviation from the expected values",
      label = "tbl:Deviations") %>% 
  pack_rows(index = c("Maximum Deviation (%)" = 3, "Averaged Absolute Deviation (%)" = 3)) %>%
  kable_styling(full_width = TRUE)
```



# Parametrizations of the Beta distributions

## Classic

According to @Pouillot2010a this equation is from @Malcolm1959 but I was unable to find it there. Lacking a better source this is from @Pouillot2010a. That is the first reason, why I somewhat doubted the following equation. The second reason is because - [according to the author](https://www.vosesoftware.com/riskwiki/ModifiedPERTdistribution.php) - the `shape` parameter λ was introduced by @Vose2000 and Vose proposes his own set of formulas. The original publications - it seems - did not intend to change the shape parameter and assumed a fixed shape = 4. Furthermore, the classic formulas as presented by @Pouillot2010a (as shown below) are equivalent to those by @Davis2008.

$$
\begin{align*}
    \mu &= \frac{a + \lambda * b + c} {\lambda + 2} \\
    \sigma &= \frac{c - a}{\lambda + 2} \\
    \alpha &= \frac{\mu - a}{c - a} \cdot \left( \frac{(\mu - a) * (c - \mu)}{\sigma^2} - 1 \right)  \\ 
    \beta &= \alpha \cdot \frac{c - \mu}{\mu - a}
\end{align*}
$$


## Golenko-Ginzburg (GolGin)

Golenko-Ginzburg [-@GolenkoGinzburg1988; cited by @Pleguezuelo2003] proposed this parametrization:

$$
α = 1 + \lambda * \frac{b − a}{c − a} \\
β = 1 + \lambda * \frac{c − b}{c − a}
$$ Golenko-Ginzburg lost mean and variance in his equation.

## Vose

@Vose2008 seems to bring the mean back into the equations.

$$
\begin{align*}
  \alpha &= \frac{(\mu - a)(2b-a-c)}{(b-\mu)(c-a)} \\
  \beta  &= \alpha  \frac{(c - \mu)}{\mu - a}
\end{align*}
$$ However, inserting the mean this can be simplified and it turns out that approach by @Vose2008 is equivalent to @GolenkoGinzburg1988.

$$
\begin{align*}
  \mu    &= \frac{a + \lambda b + c}{\lambda+2} \\
  \alpha &= \frac{(\mu - a)(2b-a-c)}{(b-\mu)(c-a)} \\
         &= \frac{-(\lambda+1) a + \lambda b + c}{c - a}  \\
         &= 1 + \lambda \cdot \frac{b − a}{c − a} \\
  \beta  &= \alpha \cdot \frac{(c - \mu)}{\mu - a} \\
         &= \left(1 + \lambda \cdot \frac{b − a}{c − a} \right) \cdot \frac{c - \mu}{\mu - a} \\
         &= \frac{a + \lambda b - (\lambda+1) c}{a-c} \\
         &= 1 + \lambda * \frac{c − b}{c − a}
\end{align*}
$$


## Davis

@Davis2008 finally uses

$$
\begin{aligned}
  \alpha &= \frac{2(c+4b-5a)}{3(c-a)} \cdot 
            \left[ 1+4(\frac{(b-a)(c-b)}{(c-a)^2}) \right] \\
  \beta  &= \frac{2(5c-4b-a)}{3(c-a)} \cdot 
            \left[ 1+4(\frac{(b-a)(c-b)}{(c-a)^2}) \right]
\end{aligned}
$$

Comparing these three approaches, the plot below shows how the beta parameters (y axis) change depending on the position of the mode. The distribution ranges from `a = min = -1` to `b = max = 1` The position of the mode is shown on the x-axis ranges from `min` to `max`. The curves for Vose and Davis are slightly offset because some curves would be too similar to be distinguished.

There is a clear difference between the parameters proposed by Vose and Davis. The Classic parameter, however, cannot be found. It hides behind Davis' parameter curves. That fuels my doubt regarding the classic beta parameter equations proposed by @Pouillot2010a.

As predicted earlier the Golenko-Ginzburg parameters are hidden, too, and hide behind Vose's parameters.

```{r Compare parametrization, echo=FALSE}
library(ggplot2)
min  <- -1
max  <- +1
x    <- seq(min, max, 0.05)
Span <- length(x)

Methods <- c("classic", "golgin", "vose" ,"davis") #
Jitter  <- c(classic = 0, golgin = 0, vose = 0.03, davis = 0.03)
ML <- length(Methods)
Params  <- c("alpha", "beta")
PL <- length(Params)

Data <- data.frame(
  x      = rep(x, ML*PL),
  method = rep(Methods, each = Span*PL),
  param  = rep(rep(Params, each = Span), ML),
  value  = rep(0, ML * PL * Span)
)

for (m in Methods) {
  y <- Pert2BetaParams(min, x, max, 4, m)
  
  Index <- (match(m, Methods) - 1) * Span*2L + 1
  Data[["value"]][Index:(Index+Span-1)] <- y$alpha + Jitter[m]

  Index <- Index + Span
  Data[["value"]][Index:(Index+Span-1)] <- y$beta + Jitter[m]
}
ggplot(Data, aes(x = x, y = value, colour = method, linetype = param)) + 
  geom_line(size = 2/3) + theme_linedraw() + 
  scale_color_manual(values = c("sienna1", "tan4", "hotpink1", "darkmagenta")) + 
  labs(x = "Mode", y = "beta parameter")
```

Or maybe all this tells another story. Maybe @Malcolm1959 got it right in the first place and were misinterpreted later causing a lot of confusion. Let us look at a few words from @Clark1962:

> However, the beta distribution still has a free parameter after its mode and extremes are designated.

The authors are obviously fully aware of the fact that the beta distribution is still ambiguous unless further assumptions are made. So, @Clark1962 continues:

> Suppose we select one-sixth of the range as the standard deviation (the normal distribution truncated at ±2.66) has its standard deviation 1/6 fo the range, and we feel that this truncated normal distribution is an appropriate simple model for specifying the ratio of the standard deviation to the range).

Which could be the assumption capable of defining the Pert beta distribution. And - indeed - @Clark1962 finishes:

> Then a beta distribution is determined, and one can convert the mode and extremes into the expected value and variance.

That sounds like @Malcolm1959 and @Clark1962 had already done what @Davis2008 (re-?) invented 46 years later because the original intention of the authors was misunderstood. A glance at Appendix B of @US1958 seems to confirm that notion. But, of course, without further knowledge this is just a hypothesis.


# The Four-Parameter Beta Distributions

Pert is based on the Beta distribution. While the standard definition of the beta distribution has a range of $[0, 1]$ a re-parametrized version with four parameters allows setting a range $[a, c]$, too.

Mean [@enwiki1011418887; @Treat1983]

$$
\mu_1 = \frac{\alpha}{\alpha+\beta} (c - a) + a
$$

Variance [@Treat1983]

$$
\mu_2 = \frac{(c-a)^2 \alpha\beta}{(\alpha+\beta)^2 (\alpha+\beta+1)}
$$

"Since the skewness and excess kurtosis are non-dimensional quantities (as moments centered on the mean and normalized by the standard deviation), they are independent of the parameters a and c" [@enwiki1011418887]

Skewness [@Treat1983] 

$$
\mu_3 = \frac{2 (\beta-\alpha) \sqrt{\alpha+\beta+1}}{(\alpha+\beta+2)\sqrt{\alpha\beta}}
$$

Kurtosis [equations taken from @Treat1983; and @enwiki1011418887, but it is unclear if they are equivalent].

$$
\mu_4 = \frac{3 (\alpha+\beta+1) [2(\alpha+\beta)^2 + \alpha\beta (\alpha+\beta-6)]} {\alpha\beta (\alpha+\beta+2) (\alpha+\beta+3)} 
= \frac{6[(\alpha - \beta)^2 (\alpha +\beta + 1) - \alpha \beta (\alpha + \beta + 2)]}
{\alpha \beta (\alpha + \beta + 2) (\alpha + \beta + 3)} 
$$


<!--
# lsjhkfksjgfhsj

$$
a^3 + (36 r^3 - 36 r^2 + 7r)a^2 - 20 r^2 a - 24 r^3 = 0
$$
-->

# References
